% \newcommand{\bbP}{\mathbb{P}}

 

\section{Introduction}
Генерация изображений на сегодняшний день в глубинном обучении представляет собой высокую роль .Существует несколько методов , которую могут из шумового представления сгенерировать правдоподобную картинку, давайте перечислим некоторые лишь из таких методов
\begin{itemize}
    \item Адверсариальные модели
    \item Нормалитзационные потоки
    \item Автоэнкодеры
    

\end{itemize}
Мы же остановим свой взор на моделях, которые являются в текцщий момент времени  наилучшими по генерацию картинок, хоть и тратят достаточно большое время на обучении - это Диффузионные модели

\section{References Review}
Авторы работы \cite{Jasha}предложили метод для генерации изображений на основе дифузионных процессов.В своей работе они рассматривали в качестве переходных марковских ядер нормальные распределения, чье математическое ожидание и дисперсия зависили от коэффициента $\beta$. Данный коэффициент предлагалась подбирать таким образм, чтобы за азаднное число итераций метода в прямом проходе $\textbf{T}  $ мы могли получить картинку , каждый пиксель которой представляет собой нормальное распределение.Подобрав данный коэффициент можно получить представление того, как с точки зрения Гауссовских диффузионных процессов выглядит переходное марковское ядро прямого процесса. Для генерации картинок из шумового представления было необходтмо обучить обратный диффузионный процесс. Благодаря теоремы Феллера, было установлено , что марковские ядра обратного процесса из того же семейства распределений, что и собственно ядра прямого. Однако, параметры таких распределений неизвестны , тогда получается , что переходное ядро марковского процесса можно определить следующим образом
$$ r(x_{t-1}|x_{t}) = \mathcal{N}(\mu_{\phi }(x,t),\sigma^{2}_{\phi}(x,t)) $$
Тогда логично будет обучать параметры таких ядер , чтоб максимизировать правдоподобие сгенерированных и реальных картинок. Стоит здесь отметить, что 
$\mu_{\phi }(x,t), \sigma^{2}_{\phi}(x,t)$ - это две нейросети, что получают на вход тукцщцю картинку и время - пытаясь предсказать картинку на следующем шаге.
\\[0.3 cm]
Также имеется и другой ряд работ, считающие что стоит учить ядро марковское обратного процесса не как апостериорное распределение для ядра прямого соответсвующего, а как то распределение, благодаря которому по данным мы можем как можно лучше оценить градиент логарифма распределения на текущей итераци  \cite{Ermo}
Также и присутсвует другой ряд работпытающиеся зашумить манифолд или воспользоваться идее денойзинга \cite{Deno}
\section{main part}
Авторы работы ы \cite{Sde} посмотрели на предыдущите работы и заметили, что на самом деле мы рассматриваем дискретизованную картину мира. И на самом деле, можно перейти к непрерывной динамике
\\[0.2 cm]
Давайте рассмотрим такую системудискретную и постаремся ее сделать непрерывной по времени , сейчас мы иммем следующую запись для картинки генерируемой на следующем шаге
$$ x_{i} = x_{i-1} \sqrt{1 - \beta_{i}} + \sqrt{\beta_{i}}z_{i-1} $$
Сделав , следующее предположение , можно записать следующее выражение
$$ \hat{\beta_{i}} = N \beta_{i} $$
Заметив, что  $N \to \infty$тогда $\beta(t) : t \in [0,1]$
\vspace{5pt}
Пусть $\beta(\frac{i}{N}) = \hat{\beta_{i}}$,  $x(\frac{i}{N}) = x_{i}$, $z(\frac{i}{N}) = z_{i}$,$  dt = \frac{1}{N}$
$$ x(t +  dt) = x(t) \sqrt{1 - \beta(t + dt)dt} + \sqrt{\beta(t + dt)dt}z(t)  $$
$$ x(t + dt) \approx x(t) - \frac{1}{2} \beta(t + dt) x(t) dt  + \sqrt{\beta(t +dt)dt}z(t)$$
$$ dx = - \frac{1}{2} \beta(t)x dt  + \sqrt{\beta(t)}dw$$
Таким образом, сделав разложение тейлора мы видим с вами, что мы можем перейти к непрерывной динамике и таким образом получить запись для стохастического дииференциального уравнения.
\\[0.2 cm]
Давайте, вспомним запись стохастического дифференциального уравнения , которое можно записать в следующем виде
$$ dx = f(x,t)dt + G(t)dw $$
Нетрудно заметить, что мы уэе с вами получили нечто похожее пару моментов назад, и тогда можно определить
$$ f(x,t) = -\frac{1}{2}\beta(t)x(t), G(t) = \sqrt{\beta(t)} $$
Заметьте снова,что наш диффузионный процесс зависит лишь от коэффициента $\beta$  , что означает ,что снова для прямого похода нам лишь надо подобрать так этот параметр, чтобы спустя заданное число итераций наша картинка смогла получиться как семпл из стандартного нормального распределения. Но возникает однако наводящий вопрос, а как нам быть с обратным диффузионным процесс, как нам его строить. В теории стохастических дифференциальных уравнений имеется на этот счет теорема Андерсона , утвержадющая следующее
\\[0.2 cm]

$\textbf{Theorem of Anderson}$
\\[0.1 cm]
Обратный диффузионный процесс с учетом прямого процесса, описывается при помощи следующего уравнения
$$dx = [f(x,t) - \frac{1}{2}G(t)^{2}s_{\phi}(x,t)]dt  + G(t)dw $$
где $f(x,t) , G(t)$ дрифт и диффузия прямого диффузионного процесса
\\[0.3 cm]
Знание именно этой теормемы поможет нам в построении обратного диффузионного процесса, осталосьт разобраться что является 
 $s_{\phi}(x,t)$
 
Это представляет градиент логарифма плотности на каждом шаге. при прямом проходе. Таким образом, совершая прямой проход мы еще на каждом шаге при помощи такой вот нейросетевой модели обучаем градиент логарифма плотности в каждом промежуточном состоянии.

\section{Questions To Discussion}
Список вопросв по данному семинару
\begin{enumerate}
    \item  Какую роль играет Теорема Андерсона в теории стохастических дифференциальных уравнений?
    \item Что приходится учить при создании обратного стоастически-дифференциального процесса?
    \item Может ли коэффициент диффузии зависесть от координаты?
\end{enumerate}

 

